name: EKS CI/CD Pipeline

on:
  push:
    # Trigger on push to the main branch
    branches: [ main ]
  pull_request:
    # Trigger on pull request targeting the main branch
    branches: [ main ]

env:
  # AWS and ECR configuration
  AWS_REGION: ${{ secrets.AWS_REGION }}
  ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
  ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
  # EKS configuration
  KUBE_CLUSTER_NAME: ${{ secrets.KUBE_CLUSTER_NAME }}
  KUBE_DEPLOYMENT_NAME: recallrisk-deployment # Name of your Kubernetes Deployment
  KUBE_CONTAINER_NAME: recallrisk-api             # Container name in the Deployment

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Get commit SHA as unique tag
        id: set_tag
        run: |
          echo "IMAGE_TAG=$(echo ${{ github.sha }} | cut -c1-7)" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      # ... (Install dependencies and tests steps remain unchanged) ...

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # --- CD: Containerization & Push Stage ---
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Log in to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build Docker image
        run: |
          FULL_IMAGE_NAME=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          echo "FULL_IMAGE_NAME=$FULL_IMAGE_NAME" >> $GITHUB_ENV
          docker build -t $ECR_REPOSITORY:$IMAGE_TAG .

      - name: Run Local Smoke Test
        run: |
          echo "Starting local smoke test on container 'recallrisk-api'..."
          docker run -d --name recallrisk-api $ECR_REPOSITORY:$IMAGE_TAG
          sleep 5 
        
      - name: FIX: Robust Container Cleanup
        if: always()
        run: |
          docker stop recallrisk-api || true
          docker rm recallrisk-api || true
          echo "Cleanup complete."

      - name: Tag and Push Docker Image
        run: |
          docker tag $ECR_REPOSITORY:$IMAGE_TAG ${{ env.FULL_IMAGE_NAME }}
          docker push ${{ env.FULL_IMAGE_NAME }}

      # --- CD: Deployment Stage to EKS ---
      - name: Update Kubeconfig
        run: aws eks update-kubeconfig --name $KUBE_CLUSTER_NAME --region $AWS_REGION

      - name: Deploy new image to EKS
        id: deploy
        run: |
          FULL_IMAGE_NAME=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          # Update the deployment image
          kubectl set image deployment/$KUBE_DEPLOYMENT_NAME $KUBE_CONTAINER_NAME=$FULL_IMAGE_NAME
          
          echo "Deployment update initiated. Waiting for rollout..."
          # Start the rollout status command, but don't exit if it fails yet
          kubectl rollout status deployment/$KUBE_DEPLOYMENT_NAME --timeout=10m || true # Allow it to timeout without failing the workflow immediately

      - name: **Diagnose Failed Deployment**
        # This step will run ONLY if the rollout failed (exit code 1)
        if: steps.deploy.outcome == 'failure'
        run: |
          echo "Deployment failed. Collecting diagnostic information..."
          
          echo "--------------------------------------"
          echo "1. POD STATUSES:"
          echo "--------------------------------------"
          # Get status of the deployment's pods
          kubectl get pods -l app=${{ env.KUBE_DEPLOYMENT_NAME }} 
          
          echo "--------------------------------------"
          echo "2. DEPLOYMENT EVENT HISTORY (describe)"
          echo "--------------------------------------"
          # Get detailed events for the deployment (look for ReplicaSet errors)
          kubectl describe deployment/${{ env.KUBE_DEPLOYMENT_NAME }}
          
          echo "--------------------------------------"
          echo "3. RECENT POD LOGS (Check CrashLoopBackOff)"
          echo "--------------------------------------"
          # Get logs from a failed pod (this will help show the application error)
          FAILED_POD=$(kubectl get pods -l app=${{ env.KUBE_DEPLOYMENT_NAME }} -o jsonpath='{.items[0].metadata.name}')
          if [ -n "$FAILED_POD" ]; then
            kubectl logs $FAILED_POD --container ${{ env.KUBE_CONTAINER_NAME }}
          else
            echo "Could not find a pod name to pull logs."
          fi
          
          # Force the job to fail now that we have the diagnostics
          exit 1

      - name: Final Rollout Status Check (Should not run if failed)
        run: |
          echo "Rollout finished successfully!"
