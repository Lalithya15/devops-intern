name: EKS CI/CD Pipeline

on:
  push:
    # Trigger on push to the main branch
    branches: [ main ]
  pull_request:
    # Trigger on pull request targeting the main branch
    branches: [ main ]

env:
  # AWS and ECR configuration
  AWS_REGION: ${{ secrets.AWS_REGION }}
  ECR_REGISTRY: ${{ secrets.ECR_REGISTRY }}
  ECR_REPOSITORY: ${{ secrets.ECR_REPOSITORY }}
  # EKS configuration
  KUBE_CLUSTER_NAME: ${{ secrets.KUBE_CLUSTER_NAME }}
  KUBE_DEPLOYMENT_NAME: recallrisk-deployment # Name of your Kubernetes Deployment
  KUBE_CONTAINER_NAME: recallrisk-api             # Container name in the Deployment

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Get commit SHA and Cache Breaker Date
        id: set_tag
        run: |
          # Use the first 7 characters of the commit SHA as the unique image tag
          echo "IMAGE_TAG=$(echo ${{ github.sha }} | cut -c1-7)" >> $GITHUB_ENV
          # Set the current date as a cache breaker for the Dockerfile
          echo "BUILD_DATE=$(date +%Y-%m-%dT%H:%M:%S%z)" >> $GITHUB_ENV 

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # --- CD: Containerization & Push Stage ---
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Log in to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build Docker image (with cache bust)
        run: |
          FULL_IMAGE_NAME=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          echo "FULL_IMAGE_NAME=$FULL_IMAGE_NAME" >> $GITHUB_ENV
          # Pass BUILD_DATE as a build-arg to force Docker to re-evaluate the COPY steps
          docker build --build-arg BUILD_DATE=${{ env.BUILD_DATE }} -t $ECR_REPOSITORY:$IMAGE_TAG .

      - name: Run Local Smoke Test
        run: |
          echo "Starting local smoke test on container 'recallrisk-api'..."
          # Running the image temporarily
          docker run -d --name recallrisk-api $ECR_REPOSITORY:$IMAGE_TAG
          sleep 5 # Give the container a moment to start
        
      - name: Robust Container Cleanup
        if: always()
        run: |
          echo "Performing robust cleanup of container 'recallrisk-api'..."
          # Use || true to prevent job failure if container is already gone (e.g., if it crashed)
          docker stop recallrisk-api || true
          docker rm recallrisk-api || true
          echo "Cleanup complete."

      - name: Tag and Push Docker Image
        run: |
          docker tag $ECR_REPOSITORY:$IMAGE_TAG ${{ env.FULL_IMAGE_NAME }}
          docker push ${{ env.FULL_IMAGE_NAME }}

      # --- CD: Deployment Stage to EKS ---
      - name: Update Kubeconfig
        run: aws eks update-kubeconfig --name $KUBE_CLUSTER_NAME --region $AWS_REGION

      - name: Deploy new image to EKS
        id: deploy
        run: |
          FULL_IMAGE_NAME=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          
          # Update the deployment image
          kubectl set image deployment/$KUBE_DEPLOYMENT_NAME $KUBE_CONTAINER_NAME=$FULL_IMAGE_NAME
          
          echo "Deployment update initiated. Waiting for rollout (max 10m)..."
          # Allow it to timeout without failing the workflow immediately
          kubectl rollout status deployment/$KUBE_DEPLOYMENT_NAME --timeout=10m || true 

      - name: Diagnose Failed Deployment
        # This step runs ONLY if the deployment failed, collecting critical logs
        if: steps.deploy.outcome == 'failure'
        run: |
          echo "Deployment failed. Collecting diagnostic information..."
          
          echo "--------------------------------------"
          echo "1. POD STATUSES:"
          echo "--------------------------------------"
          # Get status of the deployment's pods (look for CrashLoopBackOff)
          kubectl get pods -l app=${{ env.KUBE_DEPLOYMENT_NAME }} 
          
          echo "--------------------------------------"
          echo "2. DEPLOYMENT EVENT HISTORY (describe)"
          echo "--------------------------------------"
          # Get detailed events for the deployment (look for ImagePullBackOff or failed probes)
          kubectl describe deployment/${{ env.KUBE_DEPLOYMENT_NAME }}
          
          echo "--------------------------------------"
          echo "3. RECENT POD LOGS (Check CrashLoopBackOff)"
          echo "--------------------------------------"
          # Get the name of a recently failed pod
          FAILED_POD=$(kubectl get pods -l app=${{ env.KUBE_DEPLOYMENT_NAME }} -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
          
          if [ -n "$FAILED_POD" ]; then
            echo "Attempting to pull logs from pod: $FAILED_POD"
            kubectl logs $FAILED_POD --container ${{ env.KUBE_CONTAINER_NAME }} || true
          else
            echo "Could not reliably find a pod name to pull logs. Check deployment description above."
          fi
          
          # Force the job to fail now that we have the diagnostics
          exit 1

